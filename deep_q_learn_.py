# -*- coding: utf-8 -*-
"""Deep_Q_Learn .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12RSR57BvB2A_SGCgFlUup_oG3B019XFG
"""

import gym
import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import namedtuple, deque

# Hyperparameters
gamma = 0.99
epsilon_start = 1.0
epsilon_end = 0.01
epsilon_decay = 0.995
alpha = 0.001
batch_size = 64
target_update = 10
memory_capacity = 10000
num_episodes = 1000
max_t = 1000

# Experience tuple
Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state'))

class ReplayMemory:
    def __init__(self, capacity):
        self.memory = deque(maxlen=capacity)

    def push(self, *args):
        self.memory.append(Experience(*args))

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

class DQN(nn.Module):
    def __init__(self, state_size, action_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_size, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

class Agent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = ReplayMemory(memory_capacity)
        self.epsilon = epsilon_start
        self.policy_net = DQN(state_size, action_size)
        self.target_net = DQN(state_size, action_size)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=alpha)

    def select_action(self, state):
        if random.random() > self.epsilon:
            with torch.no_grad():
                return self.policy_net(state).argmax().item()
        else:
            return random.randrange(self.action_size)

    def optimize_model(self):
        if len(self.memory) < batch_size:
            return
        experiences = self.memory.sample(batch_size)
        batch = Experience(*zip(*experiences))

        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)
        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])
        state_batch = torch.cat(batch.state)
        action_batch = torch.cat(batch.action)
        reward_batch = torch.cat(batch.reward)

        state_action_values = self.policy_net(state_batch).gather(1, action_batch.unsqueeze(1))

        next_state_values = torch.zeros(batch_size)
        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()

        expected_state_action_values = (next_state_values * gamma) + reward_batch

        loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def update_target_network(self):
        self.target_net.load_state_dict(self.policy_net.state_dict())

# Initialize the environment
env = gym.make('CartPole-v1')
state_size = env.observation_space.shape[0]
action_size = env.action_space.n
agent = Agent(state_size, action_size)

# Training the agent
for episode in range(num_episodes):
    state = env.reset()
    state = torch.tensor([state], dtype=torch.float32)
    for t in range(max_t):
        action = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)
        reward = torch.tensor([reward], dtype=torch.float32)
        next_state = torch.tensor([next_state], dtype=torch.float32) if not done else None
        agent.memory.push(state, torch.tensor([action]), reward, next_state)

        state = next_state
        agent.optimize_model()

        if done:
            break

    if episode % target_update == 0:
        agent.update_target_network()

    if agent.epsilon > epsilon_end:
        agent.epsilon *= epsilon_decay

    print(f"Episode {episode+1}/{num_episodes}, Epsilon: {agent.epsilon:.2f}")

# Evaluating the agent
num_eval_episodes = 10
total_rewards = []

for _ in range(num_eval_episodes):
    state = env.reset()
    state = torch.tensor([state], dtype=torch.float32)
    total_reward = 0
    for t in range(max_t):
        action = agent.policy_net(state).argmax().item()
        state, reward, done, _ = env.step(action)
        state = torch.tensor([state], dtype=torch.float32)
        total_reward += reward
        if done:
            break
    total_rewards.append(total_reward)

print(f"Average reward over {num_eval_episodes} evaluation episodes: {np.mean(total_rewards)}")